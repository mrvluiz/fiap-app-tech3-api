# -*- coding: utf-8 -*-
"""jUPTER_tech_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13I-kkJ2LdksPpHa7QcWAGwEUoTCX-QYi
"""

! pip install category_encoders

"""# Nova secção"""

! pip install boto3

! pip install flask

! pip install kaggle

! pip install flask_restx

import uuid
import pandas as pd
import os
import time
import subprocess
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import shutil
import json
import boto3
import pandas as pd
import io


from os import listdir
from os.path import isfile, join
from datetime import datetime
from flask import Flask, jsonify
from flask_restx import Api, Resource

pd.set_option('display.max_columns', None)

KAGGLE_DS_MEDICAL_NO_SHOW = 'joniarroba/noshowappointments/data'
KAGGLE_DATASET_NAME = 'gkalpolukcu/knn-algorithm-dataset'
KAGGLE_PATH = 'kaggle-download'
BUCKET_NAME = 'bucket-fiap-tech3-dw'
BUCKET_PATH_TRAINING = 'ML-CANCER-TRAINING'
BUCKET_PATH_TEST = 'ML-CANCER-TEST'

def GetSessionAWS ():
    session = boto3.Session(aws_access_key_id='ASIAYKRRLZFBY4QD6HVZ',
        aws_secret_access_key='1xfDk78YB947fyK2zy33TTlLI4eOxraOobXhZcIy',
        aws_session_token='IQoJb3JpZ2luX2VjEHcaCXVzLXdlc3QtMiJHMEUCIQCkJN2xkm4wLMJAa6gwoOCub5IRBpULdkvVfvfyyIg0MgIgEYe+idreoXliUBbQ5iLnw2z1BWV+IMNqrUKPOYJ+gZAqvQIIsP//////////ARABGgw1NzI0MDc5MjUwNTkiDLjEyY8lHqb4mYK4aSqRAkJIUcrSfsBBiqbXjkCpK6RSXL2MORce32ZXLZJyie6T3HLkC223Rv05A9pEHdhqaRpzj5DDlArYWLA4qUTxZtGnQC2cbXImtg8x6BXmui7Y3YZdzvHFlVnGENd8hij4HACsbB922U97WbTZj8wzD4jpxkoqDfxbCaGs/ASluwXoVRrs9Rmr6IKiYoMGH1U0CL6XrRf7PVuv8aGZgMMvlviRN2lXAWWluyPXcmh++fPwSDMdehi6G67/G2a/tTxOO7fuuyyC15LjD2FGORs70QMIxNlzwOBhOwFqYU+UQu3FxCG44N2zB6hugkSAfstIUIXQ2f8Y0BwvE5+9BascfxBeNPxH6tbLSSdMObCnpOrhTDCXssK3BjqdAYEnMja8eaL1HR0oKY3PpemHBlAgOFHCrmUF0YfCJ3WHMO5/d7jrCqTDzTv9cj3FyQHN8ZDfDayOgEC9fphXLnnPDx+zrjRazHszDNPuLKsKihmstnBiUOdlEeapK1ghBgJkDhpe0PMn48TS7MAhyZpMc+eLp2s89qQS37CeY1/JiZ/Mrdew+KdgiB+spURifW7TiwTivAQEEnElBRw='
        )
    s3 = session.resource('s3')
    return s3

def Read_From_AWS (BucketName, BucketPath):
    s3 = GetSessionAWS()
    my_bucket = s3.Bucket(BucketName)

    for my_bucket_object in my_bucket.objects.all():
        splitedKey = my_bucket_object.key.split("/")

        if splitedKey[0] == BucketPath:
            content_object =  my_bucket_object.get()['Body'].read()
            df = pd.read_csv(io.BytesIO(content_object))
            df.head()
            return df

def Send_DataSet_To_AWS (DownloadPath, BucketName, BucketPath):
    s3 = GetSessionAWS()

    files = []
    path_destiny = os.path.join(DownloadPath, BucketPath)
    arrayDir =  os.listdir(path_destiny)
    for file in arrayDir:
        file_destiny = os.path.join(path_destiny, file)
        file_key = file

        if BucketPath != "":
              file_key =   BucketPath + "/" + file_key

        with open(file_destiny, 'rb') as data:
            s3.Bucket(BucketName).put_object(Key=file_key, Body=data, Tagging='KeyStatus=RawParquet')
            files.append({'objeto':file_key, 'resultado': 'Sucesso'})

    return files

aws_path = 'ML-MEDICAL-NOSHOW'
kaggle_ds = 'joniarroba/noshowappointments'
result = Read_From_AWS(BUCKET_NAME, aws_path)

df = result

df.head( )

df.shape

print(df.columns)

df = df.drop('PatientId', axis=1)
df = df.drop('AppointmentID', axis=1)
df.insert(loc=3, column='HorasAteDataDaConsulta', value = None)
df['HorasAteDataDaConsulta'] = (pd.to_datetime(result['ScheduledDay']) - pd.to_datetime(result['AppointmentDay'])).dt.total_seconds() / 3600



df.head()

df['ScheduledDay'] = pd.to_numeric(pd.to_datetime(df['ScheduledDay']))
df['AppointmentDay'] = pd.to_numeric(pd.to_datetime(df['AppointmentDay']))

df.head()

df['Target'] = df['No-show'].replace({'No' : 0, 'Yes' : 1} )
X = df[ ['Gender', 'ScheduledDay', 'AppointmentDay', 'HorasAteDataDaConsulta',
       'Age', 'Neighbourhood', 'Scholarship', 'Hipertension', 'Diabetes',
       'Alcoholism', 'Handcap', 'SMS_received']]

y = df['Target']

categorical_features = ['Gender', 'Neighbourhood']
continous_features = ['ScheduledDay', 'AppointmentDay', 'HorasAteDataDaConsulta',
                        'Age', 'Scholarship', 'Hipertension', 'Diabetes',
                        'Alcoholism', 'Handcap', 'SMS_received']

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler
from category_encoders import CatBoostEncoder
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

continous_transformer = Pipeline (steps = [('scaler', MinMaxScaler())])

categorical_transformer = Pipeline (steps = [('encoder', CatBoostEncoder())])

preprocessor = ColumnTransformer(
    transformers = [
            ('num', continous_transformer, continous_features),
            ('cat', categorical_transformer, categorical_features)
    ]
)

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', KNeighborsClassifier(n_neighbors=3, metric = 'euclidean', weights='distance'))
])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


pipeline.fit(X_train, y_train)

def roda_modelo_com_validacao_cruzada(modelo, x_train, y_train, x_test, y_test):
    from sklearn.metrics import roc_curve, roc_auc_score, classification_report
    from sklearn.model_selection import cross_val_score, StratifiedKFold
    import matplotlib.pyplot as plt

    # Treinando modelo com os dados de treino
    modelo.fit(x_train, y_train)

    # Fazendo previsões de probabilidades para o conjunto de teste e de treino
    prob_predic_test = modelo.predict_proba(x_test)
    prob_predic_train = modelo.predict_proba(x_train)

    y_pred_probs_test = prob_predic_test[:, 1]
    y_pred_probs_train = prob_predic_train[:, 1]

    print(f"\n------------------------------Resultados {modelo}------------------------------\n")

    # Calculando a AUC (área sob a curva ROC) para o conjunto de teste e treino
    auc_test = roc_auc_score(y_test, y_pred_probs_test)
    auc_train = roc_auc_score(y_train, y_pred_probs_train)
    print(f"AUC (Teste) {auc_test:.2f}")
    print(f"AUC (Treino) {auc_train:.2f}")

    # Fazendo a predição dos dados de teste e calculando o classification report
    predicao = modelo.predict(x_test)
    print("\nClassification Report")
    print(classification_report(y_test, predicao, zero_division=0))

    print("\nRoc Curve\n")
    # Calcular a curva ROC para o conjunto de teste e treino
    fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_probs_test)
    fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_probs_train)

    # Plotar a curva ROC para o conjunto de teste e treino
    plt.figure(figsize=(8, 6))
    plt.plot(fpr_test, tpr_test, color='blue', lw=2, label=f'ROC curve Teste (AUC = {auc_test:.2f})')
    plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve Treino (AUC = {auc_train:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Taxa de Falso Positivo')
    plt.ylabel('Taxa de Verdadeiro Positivo')
    plt.title('Curva ROC para Conjuntos de Treino e Teste')
    plt.legend(loc='lower right')
    plt.show()

    # Converter probabilidades em classes preditas (0 ou 1)
    y_pred = (y_pred_probs_test > 0.5).astype(int)

    # Realizar validação cruzada de 5 folds e imprimir resultados
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(modelo, x_train, y_train, cv=skf, scoring='roc_auc')

    print("\nResultados da Validação Cruzada de 5 Folds:")
    print("AUC médio:", scores.mean())
    print("Desvio padrão AUC:", scores.std())

    # Um desvio padrão alto pode indicar que o desempenho do modelo é mais sensível à escolha específica dos conjuntos de treino
    # e teste em cada fold, enquanto um desvio padrão baixo sugere uma maior robustez.

roda_modelo_com_validacao_cruzada(pipeline, X_train, y_train, X_test, y_test)

from sklearn.ensemble import RandomForestClassifier


# Criar a pipeline completa
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

# Dividir os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Treinar a pipeline
pipeline.fit(X_train, y_train)

roda_modelo_com_validacao_cruzada(pipeline, X_train, y_train, X_test, y_test)

from sklearn.metrics import roc_curve, roc_auc_score, classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold
import matplotlib.pyplot as plt

import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Extrair o modelo treinado da pipeline
model = pipeline.named_steps['classifier']

# Extrair a importância das características
importances = model.feature_importances_

# Obter os nomes das características após o pré-processamento
feature_names = continous_features + categorical_features  # Simplesmente concatenar as listas
preprocessed_feature_names = pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['encoder'].get_feature_names_out(categorical_features)

# Combinar os nomes das características contínuas e categóricas
final_feature_names = continous_features + list(preprocessed_feature_names)

# Criar um DataFrame para visualizar as importâncias
feature_importance_df = pd.DataFrame({
    'feature': final_feature_names,
    'importance': importances
})

# Ordenar por importância
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Visualizar a importância das características
plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=feature_importance_df)
plt.title('Importância das Características')
plt.xlabel('Importância')
plt.ylabel('Característica')
plt.show()

from imblearn.under_sampling import RandomUnderSampler

# Inicializa o RandomUnderSampler
rus = RandomUnderSampler(random_state=42)

# Aplica o undersampling no conjunto de dados
X_resampled, y_resampled = rus.fit_resample(df.drop('Target', axis=1), df['Target'])

# Cria um novo DataFrame com os dados balanceados
df_balanced = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['Target'])], axis=1)

df_balanced.shape

df_balanced['Target'].value_counts()

df_balanced['Target'] = df_balanced['No-show'].replace({'No' : 0, 'Yes' : 1} )
X = df_balanced[ ['Gender', 'ScheduledDay', 'AppointmentDay', 'HorasAteDataDaConsulta',
       'Age', 'Neighbourhood', 'Scholarship', 'Hipertension', 'Diabetes',
       'Alcoholism', 'Handcap', 'SMS_received']]

y = df_balanced['Target']

# Identificar as colunas categóricas e contínuas
categorical_features = ['Gender', 'Neighbourhood']
continous_features = ['ScheduledDay', 'AppointmentDay', 'HorasAteDataDaConsulta',
                        'Age', 'Scholarship', 'Hipertension', 'Diabetes',
                        'Alcoholism', 'Handcap', 'SMS_received']

# Pré-processador para variáveis contínuas
continuous_transformer = Pipeline(steps=[
    ('scaler', MinMaxScaler())
])

# Pré-processador para variáveis categóricas
categorical_transformer = Pipeline(steps=[
    ('encoder', CatBoostEncoder())
])

# Combinar os pré-processadores usando ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', continuous_transformer, continous_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Criar a pipeline completa
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())  # Você pode trocar pelo modelo de sua escolha
])

# Dividir os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinar a pipeline
pipeline.fit(X_train, y_train)

roda_modelo_com_validacao_cruzada(pipeline, X_train, y_train, X_test, y_test)

import pickle

# Salve a pipeline
filename = 'pipeline.pkl'
pickle.dump(pipeline, open(filename, 'wb'))

df_validacao = Read_From_AWS(BUCKET_NAME, aws_path)
df_validacao.insert(loc=3, column='HorasAteDataDaConsulta', value = None)
df_validacao = df_validacao.drop('PatientId', axis=1)
df_validacao = df_validacao.drop('AppointmentID', axis=1)
df_validacao['HorasAteDataDaConsulta'] = (pd.to_datetime(result['ScheduledDay']) - pd.to_datetime(result['AppointmentDay'])).dt.total_seconds() / 3600
df_validacao['ScheduledDay'] = pd.to_numeric(pd.to_datetime(df['ScheduledDay']))
df_validacao['AppointmentDay'] = pd.to_numeric(pd.to_datetime(df['AppointmentDay']))

# Selecione as features relevantes para a predição
X_validacao = df_validacao[['ScheduledDay', 'AppointmentDay', 'HorasAteDataDaConsulta',
                          'Age', 'Scholarship', 'Hipertension', 'Diabetes',
                          'Alcoholism', 'Handcap', 'SMS_received', 'Gender', 'Neighbourhood']]

# Faça a predição usando o pipeline treinado
y_pred = pipeline.predict(X_validacao)

# Adicione as predições ao DataFrame de validação
df_validacao['Predição'] = y_pred
df_validacao.head()

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['Neighbourhood'].value_counts()
    for x_label, grp in _df_41.groupby('Gender')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('Gender')
_ = plt.ylabel('Neighbourhood')

df_validacao.to_csv('validacao.csv')